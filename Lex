# calclex.py

from sly import Lexer


class DeleteLexer(Lexer):
    tokens = {DELETE, ALL, NEXT, RECORD, REST, FOR, WHILE,
              IN, TO_ARRAY, NOOPTIMIZE, COMMA, PLUS, MINUS, MULTIPLY,
              DIVIDE, OP, CP, LEQ, GEQ, NEQ, EQ, LESS,
              GREATER, ASSIGN, AND, OR, NOT, IDENTIFIER, NUMBER, SEMICOLON,SQ,QUOTES}

    ignore = ' \t'

    DELETE = r'[dD][eE][lL][eE][tT][eE]'
    ALL = r'[aA][lL][lL]'
    NEXT = r'[nN][eE][xX][tT]'
    RECORD = r'[rR][eE][cC][oO][rR][dD]'
    REST = r'[rR][eE][sS][tT]'
    FOR = r'[fF][oO][rR]'
    WHILE = r'[wW][hH][iI][lL][eE]'
    IN = r'[iI][nN]'
    NOOPTIMIZE = r'[nN][oO][oO][pP][tT][iI][mM][iI][zZ][eE]'
    COMMA = r','
    PLUS = r'\+'
    MINUS = r'-'
    MULTIPLY = r'\*'
    DIVIDE = r'/'
    OP = r'\('
    CP = r'\)'
    LEQ = r'<='
    GEQ = r'>='
    NEQ = r'<>|!='
    EQ = r'=='
    LESS = r'<'
    GREATER = r'>'
    ASSIGN = r'='
    AND = r'[aA][nN][dD]'
    OR = r'[oO][rR]'
    NOT = r'[nN][oO][tT]'
    IDENTIFIER = r'[a-zA-Z][a-zA-Z0-9_]*'
    NUMBER = r'0|([1-9][0-9]*)'
    SEMICOLON = r';'
    SQ = r'\''
    QUOTES = r'\"'	


    def NUMBER(self, t):
        t.value = int(t.value)
        return t

    @_(r'\n+')
    def ignore_newline(self, t):
        self.lineno += len(t.value)

    def find_column(text, token):
        last_cr = text.rfind('\n', 0, token.index)
        if last_cr < 0:
            last_cr = 0
        column = (token.index - last_cr) + 1
        return column

    def error(self, t):
        print('Line %d: Bad character %r' % (self.lineno, t.value[0]))
        self.index += 1

if __name__ == '__main__':
    data = 'DELETE FOR country = \'USA\''
    lexer = DeleteLexer()
    for tok in lexer.tokenize(data):
        print(tok)

if __name__ == '__main__':
    examples = [
        'DELETE',
        'DELETE NEXT 5 FOR age * year <= 10 IN somespace',
        'DELETE ALL FOR age = 10 WHILE year != 2022 NOOPTIMIZE IN students',
        'delete rest for _year >= 2000 and _year < 2022 nooptimize',
        '?!$',
    ]

    lexer = DeleteLexer()

    for sample in examples:
        print('INPUT: ' + sample)
        for tok in lexer.tokenize(sample):
            print('type=%r, value=%r' % (tok.type, tok.value))
        print()
